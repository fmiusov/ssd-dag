{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### tensorflow_p36 environment\n",
    "\n",
    "There are several ways to run this code\n",
    "- on a SageMaker notebook (the original intent)\n",
    "- on a physical machine with a well configured dev environment\n",
    "- on a physical machine using a Docker (grilledclub/cuda-100-tf114:*)\n",
    "\n",
    "## Step 1 - Develop a train.py script\n",
    "\n",
    "This is SageMaker Script Mode.   This is relatively new and much easier than the original SageMaker design.   You need to develop a train.py program that will:\n",
    "1. run locally - that means it will run on the local resources\n",
    "2. then you will test it locally with a Docker test\n",
    "\n",
    "If it runs in these tests, then it will/should run fine when you create a SageMaker Training job.   THIS IS THE CORRECT WAY TO USE SAGEMAKER.   Don't get confused - running jobs on the local SageMaker server isn't really what it was designed for.  It is designed to take your program and send it to outside resouces (using a Docker container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker is at 1.15\n",
    "# - kernel = conda_python3\n",
    "# ! pip install tensorflow-gpu==1.14\n",
    "#\n",
    "# - kernel = conda_tensorflow_p36\n",
    "#   1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\r\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\r\n",
      "Cuda compilation tools, release 10.0, V10.0.130\r\n"
     ]
    }
   ],
   "source": [
    "# currently CUDA 10.0\n",
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n",
    "this will show you how much memory is available in the GPU.   This is important if you start getting OOM (out of memory) errors.\n",
    "\n",
    "SageMaker p2.xlarge == 10+ GB  \n",
    "Note what is available.\n",
    "\n",
    "you can run (at a terminal)    \n",
    "  $ nvidia-smi -l 1   \n",
    "to see the GPU being used during training.  On SageMaker, you'll see the GPU is about 50% busy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 10 17:40:58 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   35C    P8     1W / 260W |    682MiB / 11014MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1369      G   /usr/lib/xorg/Xorg                            40MiB |\r\n",
      "|    0      1415      G   /usr/bin/gnome-shell                          58MiB |\r\n",
      "|    0      1608      G   /usr/lib/xorg/Xorg                           299MiB |\r\n",
      "|    0      1754      G   /usr/bin/gnome-shell                         222MiB |\r\n",
      "|    0      2093      G   ...quest-channel-token=6821675485061294071    57MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your GPU\n",
    "this should verify your GPU is correct\n",
    "\n",
    "## WARNING\n",
    "this is a good test but...  \n",
    "If you run it, it may not release  the GPU memory.   I didn't figure this out fully.   When I ran it, I would get an OOM error when the model started the training cycle - even with super small batch size.   So, something is up here.   You could play around and try stopping the notebook - check nvidia-smi to verify it released the GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet Model\n",
    "Why use a MobileNet Model?  Because the end objective is a lightweight model - one that will run on a Googl Coral TPU.    This requires a quantized model (int8 - not float32).  And, you get there from a TensorFlow Lite model.  The recommended path is to start with a model structure that you know is compatible (MobileNet) then retrain on top of it.  \n",
    "1. We pull the MobileNet v1 (there is a v2 that we aren't using) trained on COCO images\n",
    "2. We train on top of it (xfer learning) with our CFA Products\n",
    "3. That generates a TensorFlow Lite model (.tflite)\n",
    "4. We will later conver .tflite to an edge TPU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project directory: /home/jayson/projects/ssd-dag\n",
      "code directory: /home/jayson/projects/ssd-dag/code\n",
      "task directory: /home/jayson/projects/ssd-dag/tasks\n"
     ]
    }
   ],
   "source": [
    "S3_TFRECORDS_PATH = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/\"\n",
    "TFRECORDS_TARBALL = \"20190718_tfrecords.tar.gz\"\n",
    "\n",
    "\n",
    "S3_MODEL_PATH = \"s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/\"\n",
    "# base model - starting point that we train on top of\n",
    "BASE_MODEL_FOLDER = \"20180718_coco14_mobilenet_v1_ssd300_quantized\"\n",
    "\n",
    "# our CFA model\n",
    "# note the COINCIDENCE - 2018-0718 vs 2019-0718, don't let this confuse you!\n",
    "CFA_MODEL_FOLDER = \"20190718_cfa_prod_mobilenet_v1_ssd300/\"\n",
    "\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "CODE = os.path.join(PROJECT, \"code\")\n",
    "TASKS = os.path.join(PROJECT, \"tasks\")\n",
    "\n",
    "print (\"project directory:\", PROJECT)\n",
    "print (\"code directory:\", CODE)\n",
    "print (\"task directory:\", TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - 1x only\n",
    "\n",
    "Get the data from s3.  You only need to pull the data once - unless of course you update it.  you'll need to pass a directory into the training job\n",
    "\n",
    "### NOTE\n",
    "still unclear if data is in the Docker or passed in with the SageMaker job  \n",
    "TODO - figure this out, it's faster to NOT put it in the Docker (code/tfrecords), it just makes the Docker step slower.   the AWS fetch when the Docker starts is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical or Docker\n",
    "# you can run the script\n",
    "# $ cd /task\n",
    "\n",
    "# check the Globals values in the script\n",
    "# $ bash local_get_s3_files.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/tfrecords/20190718_tfrecords.tar.gz to code/tfrecords/20190718_tfrecords.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# SAGEMAKER\n",
    "#  you're in top project directory\n",
    "s3_tfrecords = os.path.join(S3_TFRECORDS_PATH, TFRECORDS_TARBALL)\n",
    "print (s3_tfrecords)\n",
    "! aws s3 cp $s3_tfrecords code/tfrecords  \n",
    "\n",
    "# tarball is now in code/tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190718_tfrecords/test.tfrecord\n",
      "20190718_tfrecords/train.tfrecord\n",
      "20190718_tfrecords/val.tfrecord\n",
      "/home/jayson/projects/ssd-dag\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf code/tfrecords/$TFRECORDS_TARBALL --strip=1 -C code/tfrecords\n",
    "\n",
    "# tfrecords are all in the tfrecords/ directory\n",
    "# SageMaker likes train/test subdirectories\n",
    "# - warning - confusion with 'test' vs 'eval'\n",
    "#      I feel eval is the post train loop to evaluate the training loop - thus called val(uaion)\n",
    "#         and test is to test a model with random real-world data\n",
    "#      SageMaker calls what I call val == test\n",
    "! pwd\n",
    "! rm code/tfrecords/train/*.tfrecord* -f\n",
    "! rm code/tfrecords/val/*.tfrecord*   -f\n",
    "! rm code/tfrecords/test/*.tfrecord* -f\n",
    "\n",
    "! mv code/tfrecords/train*.* code/tfrecords/train\n",
    "! mv code/tfrecords/val*.* code/tfrecords/val\n",
    "! mv code/tfrecords/test*.* code/tfrecords/test\n",
    "\n",
    "! rm code/tfrecords/$TFRECORDS_TARBALL\n",
    "\n",
    "# tarball is gone, tfrecord files are in code/tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model - 1x only\n",
    "\n",
    "You only have to pull the model once.  This exercise will RETRAIN an existing model.  So, you need the starting point.  In this example, we are training on top of the BASE == MobileNet V1 that was trained with COCO images.   You could train on top of a CFA model - just make sure you config everything properly.\n",
    "\n",
    "Copy the model from S3.    You are coping a model from an S3 folder.  There may be a label map and config file - that would make sense so you can reproduce that model.   However, if you are training on top of this model - those files aren't useful - MAKE SURE YOU UNDERSTAND THIS.   \n",
    "\n",
    "So when you pull the model from the folder - just make sure you understand if you are re-using those meta files (e.g. reproducing a model) or or if you need something new (xfer learning).  The training process will NOT read from this download.  The training program will read the config from the code/ just to help avoid this confusion.\n",
    "\n",
    "#### CKPT\n",
    "When you retrain, the config file has a train_config / fine_tune_checkpoint attribute.  You are going to download this BASE model and put it in the code/ckpt/ directory.   The training job will start with the checkpoint file you specify.   For example:\n",
    "\n",
    "fine_tune_checkpoint: \"ckpt/model.ckpt\"\n",
    "\n",
    "#### WARNING code/ckpt/checkpoints\n",
    "When you run training, it will checkpoint to code/ckpt/checkpoints.  \n",
    "- if you train for 5000 steps, then repeat, it will do nothing basically because it will just reload the 5000 checkpoint file.\n",
    "- then you'll think you're smart and you'll remove the 5000 checkpoint file.  Not so fast bucko!\n",
    "- because then you'll discover  there is some pointer in the checkpoints/ that told the system the 5000 checkpoint exists - but now it doesn't because you just wiped it - so you'll get an error (that's difficult to figure out)\n",
    "\n",
    "just delete the checkpoints directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical or Docker\n",
    "# - you may have to delete stuff first\n",
    "# $ cd code\n",
    "# $ rm -rf models\n",
    "\n",
    "# $ cd ../tasks\n",
    "# $ bash install_tf_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.index to code/ckpt/model.ckpt.index\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/pipeline.config to code/ckpt/pipeline.config\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.meta to code/ckpt/model.ckpt.meta\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/model.ckpt.data-00000-of-00001 to code/ckpt/model.ckpt.data-00000-of-00001\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/tflite_graph.pb to code/ckpt/tflite_graph.pb\n",
      "download: s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized/tflite_graph.pbtxt to code/ckpt/tflite_graph.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# SageMaker (& Local?)\n",
    "# -- warning - something not right here\n",
    "#    I think you have to do this local or SageMaker (gotta have a base model)\n",
    "s3_model_folder = os.path.join(S3_MODEL_PATH, BASE_MODEL_FOLDER)\n",
    "! aws s3 cp $s3_model_folder code/ckpt --recursive\n",
    "\n",
    "# code/ckpt now has model.ckpt.* files\n",
    "# there is also a pipeline.config file (this one was configured for the Google Coral - you don't want it)\n",
    "# there are also some tflite files - we don't want them either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### github tensorflow/models - 1x Only\n",
    "manually git clone the FIRST TIME.   The official TensorFlow github repo has a related repo with a bunch of models, tutorials, utilities etc.   We are using them.  So clone them to this machine.   In a subsequent step, we'll get the files we need from this local copy.\n",
    "\n",
    "!! - hold it -  \n",
    "!! this doesn't make sense, try not doing this - I don't think you need to git clone  \n",
    "!! doesn't the install_tf_models.sh do all of this?  \n",
    "!! I think we no longer copy, set just clone to code/models\n",
    "!! thus, you don't need this manual git clone, just run install_tf_models.sh in the next cell\n",
    "\n",
    "\n",
    "PHYSICAL COMPUTER  \n",
    "`cd ~/projects`  \n",
    "SAGEMAKER  \n",
    "`you should be in the SageMaker directory`  \n",
    "\n",
    "#### this will put /models into ~/projects  (you'll have ~/projects/models)\n",
    "`git clone https://github.com/tensorflow/models.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- git clone ---\n",
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 34005 (delta 5), reused 22 (delta 5), pack-reused 33983\n",
      "Receiving objects: 100% (34005/34005), 512.12 MiB | 11.94 MiB/s, done.\n",
      "Resolving deltas: 100% (21790/21790), done.\n",
      "Checking out files: 100% (3012/3012), done.\n",
      "--- get the protobuf compiler ---\n",
      "--2020-02-10 18:11:16--  https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/protocolbuffers/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip [following]\n",
      "--2020-02-10 18:11:16--  https://github.com/protocolbuffers/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/c692d808-54ca-11e6-90f6-ef943b0908bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200210T231116Z&X-Amz-Expires=300&X-Amz-Signature=6d37dd954409b7157444e8c3daf5c483919e041700cfba5f698d9e098a06d3dd&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.0.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-02-10 18:11:16--  https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/c692d808-54ca-11e6-90f6-ef943b0908bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200210T231116Z&X-Amz-Expires=300&X-Amz-Signature=6d37dd954409b7157444e8c3daf5c483919e041700cfba5f698d9e098a06d3dd&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.0.0-linux-x86_64.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.14.132\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.14.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1296281 (1.2M) [application/octet-stream]\n",
      "Saving to: ‘protobuf.zip’\n",
      "\n",
      "protobuf.zip        100%[===================>]   1.24M  6.00MB/s    in 0.2s    \n",
      "\n",
      "2020-02-10 18:11:16 (6.00 MB/s) - ‘protobuf.zip’ saved [1296281/1296281]\n",
      "\n",
      "Archive:  protobuf.zip\n",
      "   creating: include/\n",
      "   creating: include/google/\n",
      "   creating: include/google/protobuf/\n",
      "  inflating: include/google/protobuf/struct.proto  \n",
      "  inflating: include/google/protobuf/type.proto  \n",
      "  inflating: include/google/protobuf/descriptor.proto  \n",
      "  inflating: include/google/protobuf/api.proto  \n",
      "  inflating: include/google/protobuf/empty.proto  \n",
      "   creating: include/google/protobuf/compiler/\n",
      "  inflating: include/google/protobuf/compiler/plugin.proto  \n",
      "  inflating: include/google/protobuf/any.proto  \n",
      "  inflating: include/google/protobuf/field_mask.proto  \n",
      "  inflating: include/google/protobuf/wrappers.proto  \n",
      "  inflating: include/google/protobuf/timestamp.proto  \n",
      "  inflating: include/google/protobuf/duration.proto  \n",
      "  inflating: include/google/protobuf/source_context.proto  \n",
      "   creating: bin/\n",
      "  inflating: bin/protoc              \n",
      "  inflating: readme.txt              \n",
      "--- compile protobufs ---\n",
      "--- clean up ---\n",
      "--- done! ---\n"
     ]
    }
   ],
   "source": [
    "# 1 time only\n",
    "\n",
    "# get the latest software\n",
    "# - git clone (to <project>/code/models)\n",
    "# - get the protobuf compiler\n",
    "# - compile the protobufs\n",
    "# - clean up\n",
    "os.chdir(TASKS)\n",
    "! ./install_tf_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local (Script) Mode Training\n",
    "\n",
    "#### -> if you know what you're doing, (you have a working SageMaker HOSTED training job) - you can jump out here!\n",
    "\n",
    "see the AWS SageMaker tutorials notably:  \n",
    "https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-eager-script-mode/tf-eager-sm-scriptmode.ipynb\n",
    "\n",
    "The point here is, you can develop a training script locally, then know (have a high degree of confidence) it will run as a SageMaker training job.   (This is relatively new, the old way was more difficult and cumbersome.)\n",
    "\n",
    "### What is Local?\n",
    "- local on THIS SageMaker Notebook (EC2) server\n",
    "  - p2.xlarge - no problem\n",
    "  - t2.medium - probably not (I think this is the same footprint as the feeble Workspace)\n",
    "- A desktop computer.\n",
    "  - works great on an Ubuntu laptop with GPU\n",
    "  - should work on a Windows laptop if you have a python environment set up\n",
    "- An AWS Workspace - not enough memory, you'll get a memory error.   The code runs - but fails on a memory allocation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you have a training script that will run locally - without Docker?\n",
    "\n",
    "considering what is coming up, you want all code needed to train in one directory. (in this example, that will be the code/ directory.) That directory will be included in the Docker image.    \n",
    "\n",
    "This is going to get a little more cumbersome because we took a bunch of stuff from the (official) github tensorflow/models project.   - we are using the MobileNet model and a BUNCH of utilities.    To make sure we keep up to date, we will get all of this programmatically - i.e. clone the most recent version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Configuration\n",
    "\n",
    "trained-models/ may have a config file and a label map in the directories.  You can start with one of these.  BUT - there may be environmental variable values that you don't want - and you don't want the s3 pull operation to keep overwriting your config.   So, you can pull a model from s3.  Review the .config and label map files BUT !!! put YOUR config & label map file in the code/ directory.\n",
    "\n",
    "#### .config file\n",
    "See the config file for all parameters. the IN USE .config file is in the code/ direcory But you DEFINITELY need to look at these!\n",
    "- num_classes = should be consistent with labels.txt & label map\n",
    "- label_map_path (train & eval)\n",
    "    - there may be one in the model/ (that you pulled from s3)\n",
    "    - but move your desired label map to code/\n",
    "- inputs (train & eval) - not sure, SageMaker is passing that in\n",
    "- check all of the path statements \n",
    "- fine_tune_checkpoint - make sure you are fine tuning the correct file\n",
    "    - don't cross a _v1 with a _v2 - that definitely work\n",
    "   \n",
    "#### label map .pbtxt\n",
    "- classes start with 1 (not 0 based)\n",
    "- make sure your label map class count matches the config file\n",
    "- and it should match the label \n",
    "\n",
    "#### NOTE - a missing file will generate a complex error message.  NOT something as simple as file not found. \n",
    "\n",
    "#### NOTE - --model_dir parameter: \n",
    "- local mode, it needs to be model\n",
    "- SageMaker HOST mode, it needs to be /opt/ml/model\n",
    "\n",
    "--num_train_steps  \n",
    "   500 very quick test  \n",
    "   5000 more like it  \n",
    "-- num_eval_steps  \n",
    "   10 verify quick test  \n",
    "   100 more like it\n",
    "   \n",
    "beware of batch size - if you run out of GPU memory - see the config file, batch_size: 32;  you may need to decrease it if you have a small GPU\n",
    "\n",
    "GPU should be 95% utilized.  \n",
    "`nvidia-smi -l 1`\n",
    "CPU will be about 30%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(CODE)   # this will be the training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Warning !!!\n",
    "# I changed the pipeline_config_path = local*.config\n",
    "# this local version expects the data to be in code/tfrecords\n",
    "\n",
    "# sagemaker*.config\n",
    "#  uses S3 to move the data\n",
    "\n",
    "# !!! I haven't tested !!!\n",
    "\n",
    "# 20200122 - physical computer (Inspiron)\n",
    "#  using Jupyter (below) error: ModuleNotFoundError: No module named 'absl'\n",
    "#  but, ran fine from terminal\n",
    "#\n",
    "#  nvidia-smi\n",
    "#      NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. \n",
    "#      Make sure that the latest NVIDIA driver is installed and running.\n",
    "#  but it ran trained fine so CUDA was good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "--> installing: cython\n",
      "Requirement already satisfied: cython in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (0.29.15)\n",
      "--> installing: pycocotools\n",
      "Requirement already satisfied: pycocotools in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (2.0.0)\n",
      "--> installing: matplotlib\n",
      "Requirement already satisfied: matplotlib in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (3.1.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200127)\n",
      "Requirement already satisfied: six in /home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:186: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "*** train.py/main()\n",
      "*** FLAGS ***\n",
      "pipeline_config_path: local_mobilenet_v1_ssd_retrain.config\n",
      "config exists: True\n",
      "file: display.py\n",
      "file: __pycache__\n",
      "file: utils\n",
      "file: requirements.txt\n",
      "file: tflite_interpreter.py\n",
      "file: code\n",
      "file: tf_serving_inference.py\n",
      "file: cfa_utils\n",
      "file: detect.py\n",
      "file: ckpt\n",
      "file: cfa_prod_label_map.pbtxt\n",
      "file: train115.py\n",
      "file: annotation.py\n",
      "file: __init__.py\n",
      "file: local_mobilenet_v1_ssd_retrain.config\n",
      "file: model\n",
      "file: tfrecords\n",
      "file: sagemaker_mobilenet_v1_ssd_retrain.config\n",
      "file: object_detection\n",
      "file: models\n",
      "file: pipeline.config\n",
      "file: train.py\n",
      "model_dir: model\n",
      "train: tfrecords/train/train.tfrecord\n",
      "val: tfrecords/val/val.tfrecord\n",
      "sample_1_of_n_eval_examples: 1\n",
      "hparams_overrides: None\n",
      "checkpoint_dir: None\n",
      "WARNING:tensorflow:From /home/jayson/projects/ssd-dag/code/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0210 18:11:42.286212 140651022104384 deprecation_wrapper.py:119] From /home/jayson/projects/ssd-dag/code/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "checking inputs for: train_input_config\n",
      "path: True tfrecords/train/train.tfrecord\n",
      "checking inputs for: eval_input_config\n",
      "path: True tfrecords/val/val.tfrecord\n",
      " - - - - - - - - -\n",
      "WARNING:tensorflow:From train115.py:122: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n",
      "W0210 18:11:42.288809 140651022104384 deprecation_wrapper.py:119] From train115.py:122: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From train115.py:124: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0210 18:11:42.288988 140651022104384 deprecation_wrapper.py:119] From train115.py:124: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "- creating Estimator -\n",
      "checkpoint_dir: None\n",
      "- creating train_spec & eval_spec\n",
      "- train & evaluate\n",
      "- IF YOU GET NOTHING AFTER THIS, verify num_training_steps > largest checkpoint in /model\n",
      "Traceback (most recent call last):\n",
      "  File \"train115.py\", line 186, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"train115.py\", line 181, in main\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
      "    return self.run_local()\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1185, in _train_model_default\n",
      "    input_fn, ModeKeys.TRAIN))\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\n",
      "    self._call_input_fn(input_fn, mode))\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\n",
      "    return input_fn(**kwargs)\n",
      "  File \"/home/jayson/projects/ssd-dag/code/object_detection/inputs.py\", line 466, in _train_input_fn\n",
      "    params=params)\n",
      "  File \"/home/jayson/projects/ssd-dag/code/object_detection/inputs.py\", line 569, in train_input\n",
      "    batch_size=params['batch_size'] if params else train_config.batch_size)\n",
      "  File \"/home/jayson/projects/ssd-dag/code/object_detection/builders/dataset_builder.py\", line 141, in build\n",
      "    config.input_path[:], input_reader_config)\n",
      "  File \"/home/jayson/projects/ssd-dag/code/object_detection/builders/dataset_builder.py\", line 86, in read_dataset\n",
      "    sloppy=config.shuffle))\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1853, in apply\n",
      "    return DatasetV1Adapter(super(DatasetV1, self).apply(transformation_func))\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1290, in apply\n",
      "    dataset = transformation_func(self)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py\", line 94, in _apply_fn\n",
      "    buffer_output_elements, prefetch_input_elements)\n",
      "  File \"/home/jayson/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py\", line 253, in __init__\n",
      "    **self._flat_structure)\n",
      "AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure'\n"
     ]
    }
   ],
   "source": [
    "# These parameters can be set, if ommitted, takes values from SM_CHANNEL_ {_MODEL_DIR, _TRAIN, _VAL}\n",
    "# --model_dir\n",
    "# --train\n",
    "# --val\n",
    "# \n",
    "\n",
    "! python train115.py \\\n",
    "  --pipeline_config_path=\"local_mobilenet_v1_ssd_retrain.config\" \\\n",
    "  --num_train_steps=\"6000\" \\\n",
    "  --num_eval_steps=\"200\"  \\\n",
    "  --model_dir='model' \\\n",
    "  --train='tfrecords/train/train.tfrecord' \\\n",
    "  --val='tfrecords/val/val.tfrecord'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Model Output -- IMPORTANT\n",
    "Where did it go? - THERE IS A BIG DIFFERENCE BETWEEN LOCAL TRAIN AND HOSTED TRAIN -- important !!\n",
    "\n",
    "train*.py will put the output in code/model    This is true for local or SageMaker hosted trained.   In this case, you trained locally, so the output is in code/model  -- end of story.\n",
    "\n",
    "\n",
    "When you train with a SageMaker Hosted train, the output still goes to code/model -- HOWEVER - that is in a docker image (that you will never see).  Then it gets coped to S3.   Then the notebook (TrainModel_Step3_TrainingJob) pulls a model output from S3.   Then extracts the tarball to {PROJECT}/trained_model   SO AT THIS POINT THE OUTPUT IS IN A DIFFERENT LOCATION !!\n",
    "\n",
    "The convert graph script is pulling from {PROJECT}/trained_model (not the native code/model location).    The easiest solution (you will see below) is to copy the desired checkpoint graph to the {PROJECT}/trained_model location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -la  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "1. if you run for 500 steps, then rerun the exact process, it is going to restore /ckpt/checkpoints (ckpt-500) and then thinks it is done.  So, basically does nothing\n",
    "2. Don't delete ckpt/  (rm ckpt/*.*) WITHOUT removing ckpt/checkpoints/   The program is always checking that checkpoints subdirectory and trying to restore.  For exampmle, you delete ckpt/ but leave ckpt/checkpoints, it finds a reference to ckpt-500 but you just deleted it - so it aborts\n",
    "3. Always check your files & paths carefully - the error messages that get thrown with a missing file are not always clear - and my send you on a wild goose chase when in reality - it was just a missing file\n",
    "4. can't import nets - this is a PATH problem (models/research/slim needs to be in your path) - in the train.py program, it's programmatically added\n",
    "5. OOM when allocating tensor of shape [32,19,19,512] and type float\n",
    "\t [[{{node gradients/zeros_97}}]] -- go to the config file and change batch size to be smaller (e.g. 16)\n",
    "6. AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a useable model\n",
    "At this point you have checkpoint files.   You need models (graphs).   There are many flavors:\n",
    "    - saved graph\n",
    "    - frozen graph\n",
    "    - TensorFlow Lite\n",
    "    - TensorRT\n",
    "    - EdgeTPU\n",
    "    \n",
    "The notebook:  TrainingJob_Step3_TrainingJob will show you how to convert a checkpoint file to a graph (frozen graph & tflite).   There is a bash file to do this.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure NUM_TRAINING_STEPS = the max number in the checkpoint files you listed above\n",
    "#  e.g. \n",
    "# ls model\n",
    "# -rw-rw-r--  1 ec2-user ec2-user 41116528 Jan 28 15:16 model.ckpt-6000.data-00000-of-00001\n",
    "# -rw-rw-r--  1 ec2-user ec2-user    27275 Jan 28 15:16 model.ckpt-6000.index\n",
    "# -rw-rw-r--  1 ec2-user {ec2-user  6987305 Jan 28 15:16 model.ckpt-6000.meta\n",
    "NUM_TRAINING_STEPS = 6000\n",
    "! cp {CODE}/model/*{NUM_TRAINING_STEPS}* {PROJECT}/trained_model\n",
    "! ls {PROJECT}/trained_model/*{NUM_TRAINING_STEPS}*\n",
    "\n",
    "# get the config from the train*.py parameters above\n",
    "PIPELINE_CONFIG = 'local_mobilenet_v1_ssd_retrain.config'\n",
    "! ls {CODE}/{PIPELINE_CONFIG}\n",
    "\n",
    "# if you don't see your checkpoint in */trained_model/  STOP - and fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(TASKS)  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num {NUM_TRAINING_STEPS} --pipeline_config {PIPELINE_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow FROZEN GRAPH\n",
    "! ls {PROJECT}/tensorflow_model -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Lite model\n",
    "! ls {PROJECT}/tflite_model -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf114)",
   "language": "python",
   "name": "tf114"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
